{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5820df57-ff2d-4440-a1c6-665ddd18c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_KEY = 'hsdfsddsfgdsf'  # replace with your own token: huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0e391-70bc-4a9f-8fa4-0c61c608b181",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Grabbing a dataset from the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a1ffe6-da43-4b7e-9982-36628ee88e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f921507aed4290bdd230f52c2877ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/288 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57dac22da89435c87867cfa0e2727a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e399cbc8f7b4cbc908b0f5f96b21846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# https://huggingface.co/datasets/urvog/llama2_transcripts_healthcare_callcenter\n",
    "callcenter_dataset = load_dataset(\"urvog/llama2_transcripts_healthcare_callcenter\")\n",
    "\n",
    "callcenter_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b33c30-3312-4f54-b711-055d519f9028",
   "metadata": {},
   "source": [
    "![data card](../images/callcenter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55262272-958b-4f4b-86ce-1c4f372e2d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<s>[INST] Classify the following call transcript:\\n\\nAgent 3: Thank you for calling HealthHarbor, my name is Agent 3. How can I assist you today?\\n\\nCustomer: Hi Agent 3, my name is Emma Johnson. I've been experiencing some symptoms lately and I wanted to seek medical advice or get a symptom assessment.\\n\\nAgent 3: I'm sorry to hear that, Emma. I'll do my best to help you. Can you please describe the symptoms you've been experiencing?\\n\\nCustomer: Sure. I've been having a persistent headache for the past few days, and it's been accompanied by dizziness and occasional nausea. I'm not sure what could be causing it.\\n\\nAgent 3: I understand your concern, Emma. Headaches can have various causes. Have you experienced any recent changes in your lifestyle or any other symptoms besides the headache, dizziness, and nausea?\\n\\nCustomer: No major lifestyle changes, but I have noticed that my vision seems a bit blurry at times. And I've been feeling more fatigued than usual.\\n\\nAgent 3: Thank you for sharing that information, Emma. Blurry vision and fatigue can also be related to your symptoms. It's important to consider all these factors for a proper assessment. Based on your symptoms, I would recommend consulting with a healthcare professional. They will be able to conduct a thorough examination and provide a more accurate diagnosis.\\n\\nCustomer: I was hoping to get some advice before scheduling a doctor's appointment. Is there anything I can do to relieve these symptoms in the meantime?\\n\\nAgent 3: While I'm not a doctor, I can offer some general suggestions. You could try applying a cold or warm compress to your forehead to see if it helps with the headache. It's also important to stay hydrated and get enough rest. However, it's crucial to understand that these measures may not address the underlying cause of your symptoms. A medical professional will be able to provide a more targeted approach.\\n\\nCustomer: I understand. I'll make sure to schedule an appointment with a doctor as soon as possible. Thank you for your advice.\\n\\nAgent 3: You're welcome, Emma. It's always better to be safe and have a professional evaluation. Is there anything else I can assist you with today?\\n\\nCustomer: No, that's all for now. Thank you for your help, Agent 3.\\n\\nAgent 3: You're welcome, Emma. I hope you feel better soon. If you have any more questions or concerns, don't hesitate to reach out. Take care!\\n\\nCustomer: Thank you, Agent 3. Have a great day!\\n\\nAgent 3: You too, Emma. Goodbye! [/INST] Medical Advice or Symptom Assessment </s>\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callcenter_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0771bc6-c4ae-4de7-9f0e-ee63d86f1883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Classify the following call transcript:\n",
      "\n",
      "Agent 3: Thank you for calling HealthHarbor, my name is Agent 3. How can I assist you today?\n",
      "\n",
      "Customer: Hi Agent 3, my name is Emma Johnson. I've been experiencing some symptoms lately and I wanted to seek medical advice or get a symptom assessment.\n",
      "\n",
      "Agent 3: I'm sorry to hear that, Emma. I'll do my best to help you. Can you please describe the symptoms you've been experiencing?\n",
      "\n",
      "Customer: Sure. I've been having a persistent headache for the past few days, and it's been accompanied by dizziness and occasional nausea. I'm not sure what could be causing it.\n",
      "\n",
      "Agent 3: I understand your concern, Emma. Headaches can have various causes. Have you experienced any recent changes in your lifestyle or any other symptoms besides the headache, dizziness, and nausea?\n",
      "\n",
      "Customer: No major lifestyle changes, but I have noticed that my vision seems a bit blurry at times. And I've been feeling more fatigued than usual.\n",
      "\n",
      "Agent 3: Thank you for sharing that information, Emma. Blurry vision and fatigue can also be related to your symptoms. It's important to consider all these factors for a proper assessment. Based on your symptoms, I would recommend consulting with a healthcare professional. They will be able to conduct a thorough examination and provide a more accurate diagnosis.\n",
      "\n",
      "Customer: I was hoping to get some advice before scheduling a doctor's appointment. Is there anything I can do to relieve these symptoms in the meantime?\n",
      "\n",
      "Agent 3: While I'm not a doctor, I can offer some general suggestions. You could try applying a cold or warm compress to your forehead to see if it helps with the headache. It's also important to stay hydrated and get enough rest. However, it's crucial to understand that these measures may not address the underlying cause of your symptoms. A medical professional will be able to provide a more targeted approach.\n",
      "\n",
      "Customer: I understand. I'll make sure to schedule an appointment with a doctor as soon as possible. Thank you for your advice.\n",
      "\n",
      "Agent 3: You're welcome, Emma. It's always better to be safe and have a professional evaluation. Is there anything else I can assist you with today?\n",
      "\n",
      "Customer: No, that's all for now. Thank you for your help, Agent 3.\n",
      "\n",
      "Agent 3: You're welcome, Emma. I hope you feel better soon. If you have any more questions or concerns, don't hesitate to reach out. Take care!\n",
      "\n",
      "Customer: Thank you, Agent 3. Have a great day!\n",
      "\n",
      "Agent 3: You too, Emma. Goodbye! [/INST] Medical Advice or Symptom Assessment </s>\n"
     ]
    }
   ],
   "source": [
    "print(callcenter_dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609e231-8fdf-43c9-a079-a80530165680",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Grabbing a model from the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23733813-a8e1-40ac-b863-431d7b56b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer  # the auto classes automatically get the right python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe24465-1454-4afb-a53b-caafc697f11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_ID = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)  # is an instance of BertTokenizerFast\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578eb4e7-6536-4b59-9ac0-b9959603781d",
   "metadata": {},
   "source": [
    "![model card](../images/bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cbfc0dc-8dda-4939-afda-e795393885bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 4931, 2045, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('hey there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42c0d951-6f21-4efc-9821-91b381477a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[CLS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb3db3d2-bf5f-4b48-9fa5-702dd8c678fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7668, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Cafe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc038f09-95f6-40db-8c22-4c4c3547792d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7668, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('cafe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca69b1-c2bd-4147-8b58-4bc0e216e939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7bec5c1-ced2-4cb4-af30-80acaf2a698e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(MODEL_ID)  # is an instance of  BertModel\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b8ec5-88a4-4c1f-8a7e-e2f61632950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full link to the types of pipelines can be found here: https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4313f18b-a1c4-4064-8301-8f47a230c48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8544e7eb5e24912a50558079f8cce34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee33406e14e4ae9b497d8c2e3e1e787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6089df1730654325882bba7dd36aef45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd00c8e0634b49d7b0ce43b1e9fd1fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5ec2d2146d47f9937a4ef6e0899566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline  # the pipeline object lets us use fine-tuned models off the shelf\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f428fee6-7806-4a22-a1ef-b0b14fdcef3b",
   "metadata": {},
   "source": [
    "![model card](../images/finbert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "870780fe-a47b-4a4c-835b-bb5309f38935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.8791133165359497},\n",
       " {'label': 'negative', 'score': 0.9381498694419861}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\n",
    "    [\n",
    "        'Stocks rallied as the dollar grew in strength',\n",
    "        '$AAPL down heavily in the after hours'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ecf29f-6fa7-4ef6-899a-e91bf9e51d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849201d-d971-4ee5-be54-fda73cec745c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751631f-4b4c-4854-914e-c3a8abbdbea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edfb7aaa-e393-4fd4-92bd-5b0b0c16744e",
   "metadata": {},
   "source": [
    "# Toxic Classifier\n",
    "\n",
    "Using the Inference Endpoint (this one is a bert model fine-tuned to display if a comment is toxic or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba13be-ece9-41da-96b0-6e84346505e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ce29f0-0242-4481-ad99-2f29544c32e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Toxic', 'score': 0.6511591076850891}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://d2q5h5r3a1pkorfp.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "\t\"Accept\" : \"application/json\",\n",
    "\t\"Authorization\": f\"Bearer {HF_KEY}\",\n",
    "\t\"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "output = query({\n",
    "    \"inputs\": ['You are such a loser'],\n",
    "    \"parameters\": {}\n",
    "})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd006ef5-0bce-4b3d-9261-fa63364b6457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e5d7566-da6c-4c56-a4ed-71218402fb17",
   "metadata": {},
   "source": [
    "## Using Inference with [Together AI](https://together.ai/) to use DeepSeek R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96dc0e61-3b2e-4465-9037-3bf8b42610dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.28.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import huggingface_hub; huggingface_hub.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2b30c14-ff0c-49d5-9be3-3040efc13b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "\tprovider=\"together\",\n",
    "\tapi_key=HF_KEY\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f548557d-1e42-4925-b86c-191ad77ed773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the capital of France. Let me start by recalling what I know. France is a country in Europe, right? I remember that Paris is a major city there. But wait, is Paris actually the capital? Sometimes capitals can change or maybe there's a common misconception. Let me think.\n",
      "\n",
      "I've heard people talk about Paris being the city of lights, and it's famous for landmarks like the Eiffel Tower and the Louvre Museum. Also, in history classes, Paris was often mentioned in the context of French revolutions and political events. That makes me think it's the center of government, which would support it being the capital.\n",
      "\n",
      "But to be sure, maybe I should cross-check. Other countries sometimes have capitals that aren't their most famous cities. For example, the capital of the USA is Washington D.C., not New York. So maybe France has a different capital? Wait, no, I think in this case, Paris is both the largest city and the capital. Let me think of other European countries. Germany's capital is Berlin, Spain's is Madrid, Italy's is Rome. These are all major cities as well. So it's possible that Paris follows the same pattern.\n",
      "\n",
      "Another way to verify: if I think about French government institutions. The President of France resides in the Élysée Palace, which is in Paris. The National Assembly and the Senate are also located in Paris. That definitely points to Paris being the seat of government, hence the capital.\n",
      "\n",
      "I don't recall any recent changes in France's capital. Some countries have moved their capitals, like Nigeria moving from Lagos to Abuja, but France hasn't done that. So unless there's some historical change I'm missing, Paris should still be the capital.\n",
      "\n",
      "Wait, during World War II, the Vichy regime was in control of part of France, but I think Paris remained the capital even then, though occupied. After the war, the government returned to Paris. So no, there's no indication of a change there.\n",
      "\n",
      "In summary, all the evidence I can think of points to Paris being the capital of France. It's the largest city, the center of government, and widely recognized as such in culture and history. I don't see any conflicting information that would suggest otherwise. So I'm pretty confident the answer is Paris.\n",
      "</think>\n",
      "\n",
      "The capital of France is **Paris**. As the country's largest city, Paris serves as the political, cultural, and economic hub. It houses key government institutions such as the Élysée Palace (residence of the President), the National Assembly, and the Senate, solidifying its status as the capital. Historical and contemporary references consistently confirm Paris's role as France's capital, with no evidence of any official change.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d979a93b-bcf7-47e4-99e9-b917c69c28ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: QdZILr)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/deepseek-ai/DeepSeek-R1/v1/chat/completions.\nMake sure your token has the correct permissions.\nThe model deepseek-ai/DeepSeek-R1 is too large to be loaded automatically (688GB > 10GB).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/deepseek-ai/DeepSeek-R1/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceClient\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(\n\u001b[1;32m      4\u001b[0m \t\u001b[38;5;66;03m# provider=\"together\",  without together yields error because HF doesn't have an inference endpoint for this model \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \tapi_key\u001b[38;5;241m=\u001b[39mHF_KEY\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/DeepSeek-R1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:970\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    943\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    962\u001b[0m }\n\u001b[1;32m    963\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[1;32m    964\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    965\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    968\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m    969\u001b[0m )\n\u001b[0;32m--> 970\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:327\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.3.5/libexec/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:468\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    471\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: QdZILr)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/deepseek-ai/DeepSeek-R1/v1/chat/completions.\nMake sure your token has the correct permissions.\nThe model deepseek-ai/DeepSeek-R1 is too large to be loaded automatically (688GB > 10GB)."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "\t# provider=\"together\",  without together yields error because HF doesn't have an inference endpoint for this model \n",
    "\tapi_key=HF_KEY\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=1024,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
